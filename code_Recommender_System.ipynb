{
 "metadata": {
  "name": "",
  "signature": "sha256:a896c298bcedb71c42521f10136b6f69539d327e81141324448577fef5d7d3a8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from urllib2 import HTTPError\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "import numpy as np\n",
      "import math\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the folder used to store offline indexing\n",
      "# if you want to download the information data repository, please put the fold path here.\n",
      "cd "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "News"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#http://www.reddit.com/r/news/\n",
      "\n",
      "web1 = 'http://www.reddit.com/r/news/'\n",
      "\n",
      "def RedditUrls(Address):\n",
      "    html = urllib2.urlopen(Address).read()\n",
      "    soup = BeautifulSoup(html)\n",
      "    links = soup.findAll(\"a\")\n",
      "    t = set([ link[\"href\"] for link in links if link.has_attr('href')])\n",
      "    #html.has_attr checks whether a given key exists in the g.attrs dictionary \n",
      "    #and if so then it returns specified child elements.\n",
      "    \n",
      "    l = []\n",
      "    for s in t:\n",
      "        a = s.split('/')\n",
      "        if ('www.reddit.com' not in a) and ('http:' in a):\n",
      "            #and a[1]=='2015'\n",
      "            l.append(s)\n",
      "    return l\n",
      "\n",
      "l_web1 = RedditUrls(web1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(l_web1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "39"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#http://www.washingtonpost.com/business/technology/\n",
      "\n",
      "web2 = 'http://www.washingtonpost.com/business/technology/'\n",
      "\n",
      "def washingtonUrls(Address):\n",
      "    html = urllib2.urlopen(Address).read()\n",
      "    soup1 = BeautifulSoup(html)\n",
      "    soup2 = soup1.findAll('h2')\n",
      "    #links = soup1.findAll('a', attrs={'class':''})\n",
      "    links = []\n",
      "    for p in soup2:\n",
      "        a = p.findAll('a')\n",
      "        links += a\n",
      "\n",
      "    t = set([ link[\"href\"] for link in links if link.has_attr('href')])\n",
      "    #html.has_attr checks whether a given key exists in the g.attrs dictionary \n",
      "    #and if so then it returns specified child elements.\n",
      "    \n",
      "    l = []\n",
      "    for s in t:\n",
      "        if s[0] == '/':\n",
      "            s = 'http://www.washingtonpost.com' + s\n",
      "        l.append(s)\n",
      "    return l\n",
      "\n",
      "l_web2 = washingtonUrls(web2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(l_web2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "24"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getNews(articleUrl):\n",
      "    clean = lambda s: str(re.sub('[\\W_]+', ' ', s))\n",
      "    \n",
      "    try:\n",
      "        html = urllib2.urlopen(articleUrl).read()\n",
      "        soup = BeautifulSoup(html)\n",
      "        \n",
      "        title = list(set(soup.body.findAll('h1')))\n",
      "        title_clean = ' '.join(clean(s.text) for s in title).strip()\n",
      "        \n",
      "        article = soup.body.findAll('p')\n",
      "        article_clean = ' '.join(clean(s.text) for s in article).strip()\n",
      "        \n",
      "        return title_clean, article_clean\n",
      "    \n",
      "    except HTTPError:\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_link = {} \n",
      "news_content = {} \n",
      "\n",
      "for i in l_web1:\n",
      "    try:\n",
      "        title_i, article_i = getNews(i)\n",
      "        news_link[title_i] = i\n",
      "        news_content[title_i] = article_i\n",
      "    except TypeError:\n",
      "        pass\n",
      "        \n",
      "for j in l_web2:\n",
      "    try:\n",
      "        title_j, article_j = getNews(j)\n",
      "        news_link[title_j] = j\n",
      "        news_content[title_j] = article_j\n",
      "    except TypeError:\n",
      "        pass\n",
      "    \n",
      "if '' in news_link.keys():\n",
      "    del news_link['']\n",
      "    del news_content['']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(news_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "51"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Job"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#http://chicago.craigslist.org/search/jjj\n",
      "\n",
      "web3 = 'http://chicago.craigslist.org/search/jjj?query='\n",
      "\n",
      "def jobUrls(Address):\n",
      "    html = urllib2.urlopen(Address).read()\n",
      "    soup1 = BeautifulSoup(html)\n",
      "    soup2 = soup1.findAll('p', attrs={'class':'row'})\n",
      "    links = []\n",
      "    for p in soup2:\n",
      "        a = p.findAll('a', attrs = {'class':'hdrlnk'})\n",
      "        links += a\n",
      "\n",
      "    t = set([ link[\"href\"] for link in links if link.has_attr('href')])\n",
      "    #html.has_attr checks whether a given key exists in the g.attrs dictionary \n",
      "    #and if so then it returns specified child elements.\n",
      "    \n",
      "    l = []\n",
      "    for s in t:\n",
      "        ss = 'http://chicago.craigslist.org' + s\n",
      "        l.append(ss)\n",
      "    return l\n",
      "\n",
      "l_web3 = []\n",
      "for extend in ['web+design', 'tech', 'film']:\n",
      "    web = web3 + extend\n",
      "    l_result = jobUrls(web)\n",
      "    l_web3 += l_result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(l_web3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "300"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getJob(articleUrl):\n",
      "    clean = lambda s: str(re.sub('[\\W_]+', ' ', s))\n",
      "    \n",
      "    html = urllib2.urlopen(articleUrl).read()\n",
      "    soup = BeautifulSoup(html)\n",
      "    \n",
      "    jobtitle = soup.body.findAll('h2', attrs={'class':'postingtitle'})\n",
      "    jobtitle_clean = ' '.join(clean(s.text) for s in jobtitle).strip()\n",
      "    \n",
      "    jobdescribe = soup.body.findAll('section')\n",
      "    jobdescribe_clean = ' '.join(clean(s.text) for s in jobdescribe).strip()\n",
      "    \n",
      "    return jobtitle_clean, jobdescribe_clean"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_link = {}\n",
      "job_content = {}\n",
      "for i in l_web3:\n",
      "    jobtitle, jobDscb = getJob(i)\n",
      "    job_link[jobtitle] = i\n",
      "    job_content[jobtitle] = jobDscb\n",
      "\n",
      "if '' in job_link.keys():\n",
      "    del job_link['']\n",
      "    del job_content['']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(job_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "241"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Book"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bookUrls(Address):\n",
      "    html = urllib2.urlopen(Address).read()\n",
      "    soup1 = BeautifulSoup(html)\n",
      "    soup2 = soup1.findAll(\"p\", attrs={'class':'title'})\n",
      "    links = []\n",
      "    for p in soup2:\n",
      "        a = p.findAll('a', attrs = {'class':'subtle'})\n",
      "        links += a\n",
      "\n",
      "    t = set([ link[\"href\"] for link in links if link.has_attr('href')])\n",
      "    #html.has_attr checks whether a given key exists in the g.attrs dictionary \n",
      "    #and if so then it returns specified child elements.\n",
      "    \n",
      "    l = []\n",
      "    for s in t:\n",
      "        l.append(s)\n",
      "    return l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "web4_1 = 'http://www.barnesandnoble.com/s/data-analysis?csrfToken=VPugo7njqpcBlOKBK4d3vl50AB2CoVUZ&dref=838&size=90&sort=R'\n",
      "web4_2 = 'http://www.barnesandnoble.com/s/web-design?csrfToken=VPugo7njqpcBlOKBK4d3vl50AB2CoVUZ&dref=838&size=90&sort=R'\n",
      "web4_3 = 'http://www.barnesandnoble.com/s/film-produce?csrfToken=VPugo7njqpcBlOKBK4d3vl50AB2CoVUZ&dref=838&size=90&sort=R'\n",
      "\n",
      "l_web4=[]\n",
      "for web in [web4_1, web4_2, web4_3]:\n",
      "    for extend in ['', '&startat=91']: #collect book from first 3 pages, each containing 90 books\n",
      "        web_e = web + extend\n",
      "        l_result = bookUrls(web_e)\n",
      "        l_web4 += l_result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(l_web4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "260"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getBook(articleUrl):\n",
      "    clean = lambda s: str(re.sub('[\\W_]+', ' ', s))\n",
      "    \n",
      "    html = urllib2.urlopen(articleUrl).read()\n",
      "    soup = BeautifulSoup(html)\n",
      "    \n",
      "    title = soup.body.findAll('h1', attrs = {'class':'milo'})\n",
      "    booktitle = ' '.join(clean(s.text) for s in title).strip()\n",
      "    \n",
      "    overview = soup.body.findAll('div', attrs = {'class':'simple-html'})\n",
      "    overviewtext = ' '.join(clean(s.text) for s in overview).strip()\n",
      "    \n",
      "    return booktitle, overviewtext\n",
      "\n",
      "book_link = {}\n",
      "book_content = {}\n",
      "for i in l_web4:\n",
      "    book, overview = getBook(i)\n",
      "    book_link[book] = i\n",
      "    book_content[book] = overview\n",
      "\n",
      "if '' in book_link.keys():\n",
      "    del book_link['']\n",
      "    del book_content['']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(book_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "210"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stemming"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stemming(mytext):\n",
      "    import httplib, urllib\n",
      "    mytext = mytext.strip()\n",
      "    params = urllib.urlencode({'input_text': mytext})\n",
      "    conn = urllib.urlopen('http://www.bmobasher.com/cgi-bin/porter-online-stop.pl?'+params)\n",
      "    \n",
      "    words = []\n",
      "    for line in conn:\n",
      "        if line.startswith('<td align=center>'):\n",
      "            words.append(line[17:-6])\n",
      "\n",
      "    words = words[2:]\n",
      "    stem_words = []\n",
      "    for i in range(len(words)):\n",
      "        if i%2 == 1:\n",
      "            stem_words.append(words[i])\n",
      "            \n",
      "    return stem_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ContentStem(content): #content is a dictionary --> {title : stemmed title+content}\n",
      "    title_doc = {}\n",
      "    for i in range(len(content)):\n",
      "        key = content.keys()[i]\n",
      "        view = content.values()[i]\n",
      "        doc = key +' '+ view\n",
      "        title_doc[key] = doc\n",
      "    \n",
      "    content_stem = {}\n",
      "    for i in range(len(title_doc)):\n",
      "        key = title_doc.keys()[i]\n",
      "        stem = stemming(title_doc.values()[i])\n",
      "        if len(stem) != 0:\n",
      "            content_stem[key] = stem\n",
      "            \n",
      "    return content_stem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# {title : stemmed title+content} including duplicated terms\n",
      "news_stem = ContentStem(news_content)\n",
      "job_stem = ContentStem(job_content)\n",
      "book_stem = ContentStem(book_content)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Store the data into offline files with JSON"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "news_link, news_content, news_stem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_file1 = open(\"news_link.json\",\"w\")\n",
      "json.dump(news_link, news_file1, indent=4)\n",
      "news_file1.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_file2 = open(\"news_stem.json\",\"w\")\n",
      "json.dump(news_stem, news_file2, indent=4)\n",
      "news_file2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 125
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "job_link, job_content, job_stem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_file1 = open(\"job_link.json\",\"w\")\n",
      "json.dump(job_link, job_file1, indent=4)\n",
      "job_file1.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_file2 = open(\"job_stem.json\",\"w\")\n",
      "json.dump(job_stem, job_file2, indent=4)\n",
      "job_file2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "book_link, book_content, book_stem"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_file1 = open(\"book_link.json\",\"w\")\n",
      "json.dump(book_link, book_file1, indent=4)\n",
      "book_file1.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_file2 = open(\"book_stem.json\",\"w\")\n",
      "json.dump(book_stem, book_file2, indent=4)\n",
      "book_file2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "If the information data repository is ready, please run from here:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Prepared functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stemming(mytext):\n",
      "    import httplib, urllib\n",
      "    mytext = mytext.strip()\n",
      "    params = urllib.urlencode({'input_text': mytext})\n",
      "    conn = urllib.urlopen('http://www.bmobasher.com/cgi-bin/porter-online-stop.pl?'+params)\n",
      "    \n",
      "    words = []\n",
      "    for line in conn:\n",
      "        if line.startswith('<td align=center>'):\n",
      "            words.append(line[17:-6])\n",
      "\n",
      "    words = words[2:]\n",
      "    stem_words = []\n",
      "    for i in range(len(words)):\n",
      "        if i%2 == 1:\n",
      "            stem_words.append(words[i])\n",
      "            \n",
      "    return stem_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stemInput(major, city, course, career, hobby):\n",
      "    input_news_raw = major+' '+city+' '+course+' '+career+' '+hobby\n",
      "    input_job_raw = major+' '+city+' '+course+' '+career+' '+career\n",
      "    input_book_raw = major+' '+course+' '+hobby\n",
      "\n",
      "    input_news = re.sub('[\\W_]+', ' ', input_news_raw).lower()\n",
      "    input_job = re.sub('[\\W_]+', ' ', input_job_raw).lower()\n",
      "    input_book = re.sub('[\\W_]+', ' ', input_book_raw).lower()\n",
      "\n",
      "    stem_news_user = stemming(input_news)  # list of terms\n",
      "    stem_job_user = stemming(input_job)\n",
      "    stem_book_user = stemming(input_book)\n",
      "    \n",
      "    return stem_news_user, stem_job_user, stem_book_user"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def querycount(stem_target_user):\n",
      "    unique_list = list(set(stem_target_user))\n",
      "    freq_target_user = {}\n",
      "    for term in unique_list:\n",
      "        freq_target_user[term] = stem_target_user.count(term)\n",
      "    return freq_target_user"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def indexing(target_stem, stem_target_user):\n",
      "\n",
      "    # combine terms in all news and query\n",
      "    allterm_raw = []\n",
      "\n",
      "    for t in target_stem.values():\n",
      "        if len(t) != 0:\n",
      "            allterm_raw.extend(t)\n",
      "    allterm_raw.extend(stem_target_user)\n",
      "    allterm = list(set(allterm_raw)) # allterm include all unique terms combining all news\n",
      "    \n",
      "    # get index of the query\n",
      "    q_target_index = [0]*len(allterm)\n",
      "    freq_target_user = querycount(stem_target_user)\n",
      "    \n",
      "    for z in freq_target_user.keys():\n",
      "        for p,q in enumerate(allterm):\n",
      "            if q == z:\n",
      "                q_target_index[p] = freq_target_user[q]\n",
      "    \n",
      "    \n",
      "    # get index of each document\n",
      "    target_index = {}\n",
      "    for x in target_stem.keys():  # x is news title\n",
      "        \n",
      "        doc_index = [0]*len(allterm)  # array for each document\n",
      "        \n",
      "        for ti in range(len(allterm)): # ti is the index of terms in allterm\n",
      "            \n",
      "            term = allterm[ti]\n",
      "\n",
      "            if term in target_stem[x]: \n",
      "                count = target_stem[x].count(term)\n",
      "                doc_index[ti] = count\n",
      "       \n",
      "        target_index[x] = doc_index\n",
      "    \n",
      "    return q_target_index, target_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tfidf(q_target_index, target_index):\n",
      "\n",
      "    doc_matrix = np.array(target_index.values())\n",
      "    querydata = np.array(q_target_index)\n",
      "\n",
      "    N = float(len(doc_matrix)) # number of documents\n",
      "    newlist = []\n",
      "\n",
      "    for i in range(len(doc_matrix.T)):\n",
      "        nk = 0\n",
      "        for freq in doc_matrix.T[i]:\n",
      "            if freq != 0:\n",
      "                nk += 1\n",
      "        \n",
      "        if nk != 0:\n",
      "            idf = math.log(N/nk, 2)\n",
      "            newlist.append(doc_matrix.T[i]*idf)\n",
      "            querydata[i] = querydata[i]*idf\n",
      "        else:\n",
      "            newlist.append(doc_matrix.T[i])\n",
      "            querydata[i] = 0\n",
      "            \n",
      "    tfidf_doc = array(newlist).T\n",
      "\n",
      "    return tfidf_doc, querydata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ranking_result(tfidf_doc, querydata, target_link): # news_matrix_tfidf, news_user_tfidf\n",
      "    \n",
      "    data = tfidf_doc\n",
      "    Q = querydata\n",
      "    \n",
      "    from numpy import linalg as la\n",
      "    norm_Q = la.norm(Q)\n",
      "    \n",
      "    cosine = []\n",
      "    for i in data:\n",
      "        norm_rank = la.norm(i)\n",
      "        sim = np.dot(Q,i)/(norm_Q*norm_rank)\n",
      "        cosine.append(sim)\n",
      "    \n",
      "    t = []\n",
      "    for x, y in enumerate(cosine):\n",
      "        name = target_link.keys()[x]\n",
      "        t.append([y, name])\n",
      "    ranking = np.array(sorted(t, key=lambda tu: tu[0], reverse=True))[0:5]\n",
      "    \n",
      "    recommendation = {} \n",
      "    for ind, nm in enumerate(ranking[:,1]):\n",
      "        recommendation[nm] = [ranking[ind,0],target_link[nm]]\n",
      "    \n",
      "    return recommendation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ranking Results "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def InfoRec(name_n, major_n, city_n, course_n, career_n, hobby_n, news_stem, job_stem, book_stem, news_link, job_link, book_link):\n",
      "    \n",
      "    new_pro = [name_n, major_n, city_n, course_n, career_n, hobby_n]\n",
      "    pro_item = ['name','major', 'city', 'course', 'career', 'hobby']\n",
      "    \n",
      "    student = student_profile[str(name_n).lower()]\n",
      "    \n",
      "    for n in range(1,6):\n",
      "        record = stemming(student[pro_item[n]]) # record = ['human', 'computer', 'interaction']\n",
      "        record += stemming(new_pro[n])\n",
      "        record_u = list(set(record))\n",
      "        student[pro_item[n]] = ' '.join(record_u)\n",
      "    \n",
      "    major = student['major']\n",
      "    city = student['city']\n",
      "    course = student['course']\n",
      "    career = student['career']\n",
      "    hobby = student['hobby']\n",
      "    \n",
      "    stem_news_user, stem_job_user, stem_book_user = stemInput(major, city, course, career, hobby)\n",
      "    \n",
      "    q_news_index, news_index = indexing(news_stem, stem_news_user)\n",
      "    q_job_index, job_index = indexing(job_stem, stem_job_user)\n",
      "    q_book_index, book_index = indexing(book_stem, stem_book_user)\n",
      "    \n",
      "    news_matrix_tfidf, news_user_tfidf = tfidf(q_news_index, news_index)\n",
      "    job_matrix_tfidf, job_user_tfidf = tfidf(q_job_index, job_index)\n",
      "    book_matrix_tfidf, book_user_tfidf = tfidf(q_book_index, book_index)\n",
      "    \n",
      "    news_ranking = ranking_result(news_matrix_tfidf, news_user_tfidf, news_link)\n",
      "    job_ranking = ranking_result(job_matrix_tfidf, job_user_tfidf, job_link)\n",
      "    book_ranking = ranking_result(book_matrix_tfidf, book_user_tfidf, book_link)\n",
      "    \n",
      "    return news_ranking, job_ranking, book_ranking"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Student profile database"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "student_profile = {'nicole li':{'major':'', 'city':'beijing', 'course':'', 'career':'', 'hobby':'read'}, \n",
      "                   'amy rubin':{'major':'human computer interaction', 'city':'los angeles', 'course':'', 'career':'', 'hobby':'new technology'},\n",
      "                   'rand paul':{'major':'digit commun media art', 'city':'new york', 'course':'', 'career':'', 'hobby':'woodwork photography movie'}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Students inputs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Dont run these samples. They are only for your reference."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sample inputs 1. \n",
      "Name: \"Nicole Li\"\n",
      "Major: \"Predictive Analytics / Data analysis and data mining\"\n",
      "city: \"Chicago\"\n",
      "course: \"Computer Science, Data Analysis and Regression, Knowledge discovery technology, Programming Data Mining Apps, Intelligent Information Retrieval, Advanced Data Analysis\"\n",
      "career: \"data analysis or database management. I want a more technical position related to data in Twitter or Facebook or any other social media companies. Consulting companies are also ideal targets.\"\n",
      "hobby: \"reading books, sports, Japanese, desiging, new technology, cooking, photography\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sample inputs 2\n",
      "name = \"Amy Rubin\"\n",
      "major = \"Human-Computer Interaction\"\n",
      "city = \"Chicago\"\n",
      "course = \"Interaction Design and Information Architecture, Digital Design, Topics in Human-Computer Interaction, Information Visualization and Infographics, Intranets and Portals, User Experience Design Practicum\"\n",
      "career = \"Web designer, Web graphic designer, Multimedia Web designers, The UI designer in Internet service providers (ISPs) and Internet consulting firms, or traditional advertising, marketing, and PR companies.\"\n",
      "hobby = \"golfing, reading novels, and watching movies, cooking\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sample inputs 3\n",
      "name = \"Rand Paul\"\n",
      "major = \"Film / Cinema Production\"\n",
      "city = \"New York, Boston\"\n",
      "course = \"NEW DIRECTONS IN FILM AND PHILOSOPHY, INTRO TO CINEMA AND NEW MEDIA, SOUND & IMAGE THEORY, WRITING FILM CRITICISM, TOPICS IN WORLD CINEMA, TOPICS IN AMERICAN CINEMA, AMERICAN FILM HISTORY 1960-1990, ANALYSIS OF FILM LANGUAGE\"\n",
      "career = \"The associate producer for film and TV works closely with a producer to develop appropriate film and TV programming and formats; however, they may be required to work on more specific elements of a production than the producer. The position includes a wide range of responsibilities, including writing and creating content, communicating between supervisors and subordinates, coordinating between multiple departments, editing, and directing.\"\n",
      "hobby = \"sports, snowboarding, video games, hiking\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      " \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Run this, and you can just copy the profile above"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "User Interface"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Your Name:\"\n",
      "name_n = raw_input()\n",
      "print \"\"\n",
      "print \"Your Major:\"\n",
      "major_n =  raw_input()\n",
      "print \"\"\n",
      "print \"Where are you living:\"\n",
      "city_n = raw_input()\n",
      "print \"\"\n",
      "print \"What courses you have taken:\"\n",
      "course_n = raw_input()\n",
      "print \"\"\n",
      "print \"What are your career interests:\"\n",
      "career_n = raw_input()\n",
      "print \"\"\n",
      "print \"What are your hobbies:\"\n",
      "hobby_n = raw_input()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Your Name:\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Your Major:\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Where are you living:\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "What courses you have taken:\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "What are your career interests:\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "What are your hobbies:\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Check which student is inputing his/her profile:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name_n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "'Rand Paul'"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "In following part, please just run the functions with this student's name. In this way, you won't be confused."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      " "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Run"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_rec_nicole, job_rec_nicole, book_rec_nicole = InfoRec(name_n, major_n, city_n, course_n, career_n, hobby_n, news_stem, job_stem, book_stem, news_link, job_link, book_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_rec_amy, job_rec_amy, book_rec_amy = InfoRec(name_n, major_n, city_n, course_n, career_n, hobby_n, news_stem, job_stem, book_stem, news_link, job_link, book_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_rec_rand, job_rec_rand, book_rec_rand = InfoRec(name_n, major_n, city_n, course_n, career_n, hobby_n, news_stem, job_stem, book_stem, news_link, job_link, book_link)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Results  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "student_profile['nicole li']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "student_profile['amy rubin']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "student_profile['rand paul']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_rec_nicole"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_rec_amy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "news_rec_rand"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_rec_nicole"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_rec_amy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_rec_rand"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_rec_nicole"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_rec_amy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book_rec_rand"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Thank you for running!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}